{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Configuration"
      ],
      "metadata": {
        "id": "_tVFYWypOCV5"
      },
      "id": "_tVFYWypOCV5"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Set plot style for better aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Define the table ID for easy reference\n",
        "TABLE_ID = \"du-hast-mich.customer_yeahmobi.zmaticoo\""
      ],
      "metadata": {
        "id": "Ikr-LLaBMC3G"
      },
      "id": "Ikr-LLaBMC3G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-Level Job Overview"
      ],
      "metadata": {
        "id": "9QXr_8kxOFwI"
      },
      "id": "9QXr_8kxOFwI"
    },
    {
      "cell_type": "code",
      "id": "EaAXq8d6ezPvBHO7ApGV7E3G",
      "metadata": {
        "tags": [],
        "id": "EaAXq8d6ezPvBHO7ApGV7E3G"
      },
      "source": [
        "sql_job_status = f\"\"\"\n",
        "SELECT\n",
        "    state,\n",
        "    COUNT(*) AS job_count\n",
        "FROM\n",
        "    `{TABLE_ID}`\n",
        "GROUP BY\n",
        "    state\n",
        "ORDER BY\n",
        "    job_count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_job_status = client.query(sql_job_status).to_dataframe()\n",
        "\n",
        "# Plotting\n",
        "ax = sns.barplot(x='state', y='job_count', data=df_job_status)\n",
        "ax.set_title('Distribution of Dataproc Job Statuses', fontsize=16)\n",
        "ax.set_xlabel('Job Status')\n",
        "ax.set_ylabel('Number of Jobs')\n",
        "for index, row in df_job_status.iterrows():\n",
        "    ax.text(index, row.job_count, row.job_count, color='black', ha=\"center\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: A high number of 'ERROR' states could indicate underlying problems with job configurations, permissions, or cluster stability.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚è±Ô∏è Runtime vs. Elapsed Time"
      ],
      "metadata": {
        "id": "XfoBIU74ScRW"
      },
      "id": "XfoBIU74ScRW"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_runtime = f\"\"\"\n",
        "SELECT\n",
        "    run_time,\n",
        "    elapsed_time,\n",
        "    (elapsed_time - run_time) AS wait_time\n",
        "FROM\n",
        "    `{TABLE_ID}`\n",
        "WHERE\n",
        "    state = 'SUCCEEDED'\n",
        "    AND run_time IS NOT NULL\n",
        "    AND elapsed_time IS NOT NULL;\n",
        "\"\"\"\n",
        "\n",
        "df_runtime = client.query(sql_runtime).to_dataframe()\n",
        "\n",
        "# Plotting the distributions\n",
        "sns.histplot(df_runtime['run_time'], color='skyblue', kde=True, label='Run Time', stat=\"density\")\n",
        "sns.histplot(df_runtime['elapsed_time'], color='red', kde=True, label='Elapsed Time', stat=\"density\")\n",
        "plt.title('Distribution of Job Run Time vs. Elapsed Time (in seconds)')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: The distributions show a significant portion of time is spent waiting. This suggests potential bottlenecks in cluster scheduling or resource availability.\")"
      ],
      "metadata": {
        "id": "A7fPrsdkOMWf"
      },
      "id": "A7fPrsdkOMWf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí∞ DCU and Shuffle Storage Consumption"
      ],
      "metadata": {
        "id": "jNGTt_R6Sh_s"
      },
      "id": "jNGTt_R6Sh_s"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_cost_metrics = f\"\"\"\n",
        "SELECT\n",
        "    milliDcuSeconds / 3600000.0 AS dcu_hours,\n",
        "    shuffleStorageGbSeconds,\n",
        "    run_time\n",
        "FROM\n",
        "    `{TABLE_ID}`\n",
        "WHERE\n",
        "    state = 'SUCCEEDED'\n",
        "    AND milliDcuSeconds > 0;\n",
        "\"\"\"\n",
        "\n",
        "df_cost = client.query(sql_cost_metrics).to_dataframe()\n",
        "\n",
        "# Scatter plot of DCU Hours vs. Runtime\n",
        "sns.scatterplot(data=df_cost, x='run_time', y='dcu_hours')\n",
        "plt.title('DCU Hours vs. Job Runtime')\n",
        "plt.xlabel('Run Time (seconds)')\n",
        "plt.ylabel('DCU Hours')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: The strong positive correlation is expected. However, jobs that lie far above the main trend line are inefficient, consuming high DCUs for their runtime. These are prime candidates for optimization.\")"
      ],
      "metadata": {
        "id": "quL94dzVR8Wl"
      },
      "id": "quL94dzVR8Wl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Executor and Driver Configuration"
      ],
      "metadata": {
        "id": "pt5QfAKiSj8n"
      },
      "id": "pt5QfAKiSj8n"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_tiers = f\"\"\"\n",
        "SELECT\n",
        "    properties.spark_spark_dataproc_executor_compute_tier AS executor_compute_tier,\n",
        "    properties.spark_spark_dataproc_executor_disk_tier AS executor_disk_tier,\n",
        "    AVG(run_time) as avg_runtime_seconds,\n",
        "    AVG(milliDcuSeconds / 3600000.0) as avg_dcu_hours,\n",
        "    COUNT(*) as job_count\n",
        "FROM\n",
        "    `{TABLE_ID}`\n",
        "WHERE\n",
        "    state = 'SUCCEEDED'\n",
        "GROUP BY\n",
        "    1, 2\n",
        "ORDER BY\n",
        "    job_count DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "df_tiers = client.query(sql_tiers).to_dataframe()\n",
        "\n",
        "print(\"Top 10 Most Common Executor Tier Configurations:\")\n",
        "display(df_tiers)\n",
        "\n",
        "print(\"\\nInsight: Analyze if jobs using 'premium' tiers justify the cost with significantly lower runtimes. If 'standard' tier jobs have acceptable performance, they may offer better cost-efficiency.\")"
      ],
      "metadata": {
        "id": "qMp9lwPbSC3F"
      },
      "id": "qMp9lwPbSC3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÄ Impact of Shuffle Partitions on Performance"
      ],
      "metadata": {
        "id": "5hHCHmVXStiN"
      },
      "id": "5hHCHmVXStiN"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_shuffle = f\"\"\"\n",
        "SELECT\n",
        "    properties.spark_spark_sql_shuffle_partitions AS shuffle_partitions,\n",
        "    AVG(shuffleStorageGbSeconds) as avg_shuffle_gb_seconds,\n",
        "    AVG(run_time) as avg_runtime_seconds\n",
        "FROM\n",
        "    `{TABLE_ID}`\n",
        "WHERE\n",
        "    state = 'SUCCEEDED'\n",
        "    AND properties.spark_spark_sql_shuffle_partitions IS NOT NULL\n",
        "    AND shuffleStorageGbSeconds > 0\n",
        "GROUP BY\n",
        "    1\n",
        "HAVING\n",
        "    COUNT(*) > 10 -- Only consider partition counts used in more than 10 jobs\n",
        "ORDER BY\n",
        "    shuffle_partitions;\n",
        "\"\"\"\n",
        "\n",
        "df_shuffle = client.query(sql_shuffle).to_dataframe()\n",
        "\n",
        "# Plotting the results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Bar plot for average shuffle\n",
        "sns.barplot(data=df_shuffle, x='shuffle_partitions', y='avg_shuffle_gb_seconds', color='g', ax=ax1, alpha=0.6)\n",
        "ax1.set_ylabel('Avg Shuffle GB-Seconds', color='g')\n",
        "ax1.tick_params(axis='y', labelcolor='g')\n",
        "ax1.set_xlabel('Spark Shuffle Partitions')\n",
        "\n",
        "# Line plot for average runtime on a second y-axis\n",
        "ax2 = ax1.twinx()\n",
        "sns.lineplot(data=df_shuffle, x='shuffle_partitions', y='avg_runtime_seconds', color='b', marker='o', ax=ax2)\n",
        "ax2.set_ylabel('Avg Runtime (seconds)', color='b')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "\n",
        "plt.title('Impact of Shuffle Partitions on Runtime and Shuffle Storage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: Look for the 'sweet spot'. The optimal number of partitions should minimize both runtime and shuffle usage. Very low or very high values often lead to poor performance. The graph can help identify a better default value for your jobs.\")"
      ],
      "metadata": {
        "id": "xoVMQdKkSuVK"
      },
      "id": "xoVMQdKkSuVK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÊåâÁÖßËøêË°åÊó∂Èó¥Âπ≥ÂùáÂàÜÈÖçÂà∞5‰∏™Ê°∂"
      ],
      "metadata": {
        "id": "dqnhppF-k31e"
      },
      "id": "dqnhppF-k31e"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_bucket_summary = f\"\"\"\n",
        "WITH JobBuckets AS (\n",
        "  SELECT\n",
        "    run_time,\n",
        "    -- Assign jobs to one of 5 buckets based on their run time\n",
        "    NTILE(5) OVER (ORDER BY run_time) AS runtime_bucket\n",
        "  FROM\n",
        "    `{TABLE_ID}`\n",
        "  WHERE\n",
        "    state = 'SUCCEEDED' AND run_time IS NOT NULL\n",
        ")\n",
        "-- Summarize the characteristics of each bucket\n",
        "SELECT\n",
        "  runtime_bucket,\n",
        "  COUNT(*) AS number_of_jobs,\n",
        "  MIN(run_time) AS min_runtime,\n",
        "  MAX(run_time) AS max_runtime\n",
        "FROM\n",
        "  JobBuckets\n",
        "GROUP BY\n",
        "  runtime_bucket\n",
        "ORDER BY\n",
        "  runtime_bucket;\n",
        "\"\"\"\n",
        "\n",
        "df_summary = client.query(sql_bucket_summary).to_dataframe()\n",
        "\n",
        "# Create a new column for the x-axis labels (runtime ranges)\n",
        "df_summary['runtime_range'] = df_summary.apply(\n",
        "    lambda row: f\"{row['min_runtime']} - {row['max_runtime']}s\", axis=1\n",
        ")\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(14, 7))\n",
        "ax = sns.barplot(x='runtime_range', y='number_of_jobs', data=df_summary, palette='viridis')\n",
        "\n",
        "ax.set_title('Number of Jobs by Runtime Range', fontsize=16)\n",
        "ax.set_xlabel('Job Runtime Range (seconds)', fontsize=12)\n",
        "ax.set_ylabel('Number of Jobs', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Add the count labels on top of each bar\n",
        "for index, row in df_summary.iterrows():\n",
        "    ax.text(index, row.number_of_jobs, row.number_of_jobs, color='black', ha=\"center\", va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: Since we used NTILE(5), the bar chart confirms that each defined runtime range contains an equal number of jobs. This method is excellent for segmenting your workload into distinct performance tiers (e.g., 'very fast', 'fast', 'medium', 'slow', 'very slow') for targeted analysis.\")"
      ],
      "metadata": {
        "id": "tGND_HGekR52"
      },
      "id": "tGND_HGekR52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÊåâÁÖßÊó•ÊúüÁúãÊØè‰∏™Ê°∂ÊØèÂ§©‰ªªÂä°Êï∞"
      ],
      "metadata": {
        "id": "LZynGWSBmeRu"
      },
      "id": "LZynGWSBmeRu"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_daily_stacked = f\"\"\"\n",
        "WITH JobBuckets AS (\n",
        "  SELECT\n",
        "    createTime,\n",
        "    -- Use NTILE(5) to create 5 buckets based on run_time ordering\n",
        "    NTILE(5) OVER (ORDER BY run_time) AS runtime_bucket\n",
        "  FROM\n",
        "    `{TABLE_ID}`\n",
        "  WHERE\n",
        "    state = 'SUCCEEDED' AND run_time IS NOT NULL\n",
        ")\n",
        "-- Summarize the daily counts for each bucket\n",
        "SELECT\n",
        "  EXTRACT(DATE FROM createTime) AS job_date,\n",
        "  runtime_bucket,\n",
        "  COUNT(*) AS number_of_jobs\n",
        "FROM\n",
        "  JobBuckets\n",
        "GROUP BY\n",
        "  job_date,\n",
        "  runtime_bucket\n",
        "ORDER BY\n",
        "  job_date,\n",
        "  runtime_bucket;\n",
        "\"\"\"\n",
        "\n",
        "df_daily = client.query(sql_daily_stacked).to_dataframe()\n",
        "\n",
        "# Pivot the data to get dates as rows and buckets as columns\n",
        "df_pivot = df_daily.pivot(index='job_date', columns='runtime_bucket', values='number_of_jobs').fillna(0)\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "ax = df_pivot.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis_r')\n",
        "\n",
        "plt.title('Daily Job Counts Stacked by Runtime Bucket', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Number of Jobs', fontsize=12)\n",
        "plt.legend(title='Runtime Bucket (1=Fastest, 5=Slowest)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: This view helps you spot trends in your daily workload. For instance, you can see if days with a high total number of jobs are composed of many quick jobs (dominated by buckets 1 and 2) or a few long-running jobs (dominated by bucket 5). A sudden increase in 'slow' jobs on a particular day might signal an issue or a change in the type of analysis being performed.\")"
      ],
      "metadata": {
        "id": "8A7QbslDlZMV"
      },
      "id": "8A7QbslDlZMV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÊØèÂ§©ÊØè‰∏™Ê°∂ÈáåÁöÑ‰ªªÂä°‰∏≠premium vs. non-premium"
      ],
      "metadata": {
        "id": "wb8EJhWomicK"
      },
      "id": "wb8EJhWomicK"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_tier_breakdown = f\"\"\"\n",
        "WITH JobBuckets AS (\n",
        "  SELECT\n",
        "    createTime,\n",
        "    properties.spark_spark_dataproc_executor_compute_tier AS executor_compute_tier,\n",
        "    -- Use NTILE(5) to create 5 buckets based on run_time ordering\n",
        "    NTILE(5) OVER (ORDER BY run_time) AS runtime_bucket\n",
        "  FROM\n",
        "    `{TABLE_ID}`\n",
        "  WHERE\n",
        "    state = 'SUCCEEDED' AND run_time IS NOT NULL\n",
        ")\n",
        "-- Summarize the daily counts for each bucket and tier\n",
        "SELECT\n",
        "  EXTRACT(DATE FROM createTime) AS job_date,\n",
        "  runtime_bucket,\n",
        "  CASE\n",
        "    WHEN executor_compute_tier = 'premium' THEN 'Premium'\n",
        "    ELSE 'Non-Premium'\n",
        "  END AS compute_tier_type,\n",
        "  COUNT(*) AS number_of_jobs\n",
        "FROM\n",
        "  JobBuckets\n",
        "GROUP BY\n",
        "  job_date,\n",
        "  runtime_bucket,\n",
        "  compute_tier_type\n",
        "ORDER BY\n",
        "  job_date,\n",
        "  runtime_bucket,\n",
        "  compute_tier_type;\n",
        "\"\"\"\n",
        "\n",
        "df_tier_breakdown = client.query(sql_tier_breakdown).to_dataframe()\n",
        "\n",
        "print(\"Daily Job Counts Broken Down by Runtime Bucket and Compute Tier:\")\n",
        "display(df_tier_breakdown.head(10))"
      ],
      "metadata": {
        "id": "jBQVqALQmw9y"
      },
      "id": "jBQVqALQmw9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the data into two DataFrames\n",
        "df_premium = df_tier_breakdown[df_tier_breakdown['compute_tier_type'] == 'Premium']\n",
        "df_non_premium = df_tier_breakdown[df_tier_breakdown['compute_tier_type'] == 'Non-Premium']\n",
        "\n",
        "# Pivot both DataFrames\n",
        "pivot_premium = df_premium.pivot(index='job_date', columns='runtime_bucket', values='number_of_jobs').fillna(0)\n",
        "pivot_non_premium = df_non_premium.pivot(index='job_date', columns='runtime_bucket', values='number_of_jobs').fillna(0)\n",
        "\n",
        "# Create a figure with two subplots, one for each tier\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "# Plot for Premium Tier\n",
        "pivot_premium.plot(kind='bar', stacked=True, colormap='Oranges', ax=ax1)\n",
        "ax1.set_title('Daily Premium Tier Jobs by Runtime Bucket', fontsize=16)\n",
        "ax1.set_ylabel('Number of Jobs')\n",
        "ax1.legend(title='Runtime Bucket (1=Fastest, 5=Slowest)')\n",
        "\n",
        "# Plot for Non-Premium Tier\n",
        "pivot_non_premium.plot(kind='bar', stacked=True, colormap='Blues', ax=ax2)\n",
        "ax2.set_title('Daily Non-Premium Tier Jobs by Runtime Bucket', fontsize=16)\n",
        "ax2.set_xlabel('Date', fontsize=12)\n",
        "ax2.set_ylabel('Number of Jobs')\n",
        "ax2.legend(title='Runtime Bucket (1=Fastest, 5=Slowest)')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: By separating the charts, you can easily compare workload patterns. For example, you might discover that your longest-running jobs (bucket 5) are predominantly run on non-premium hardware. This could be an opportunity for optimization: testing if running these slow jobs on the premium tier would be more cost-effective by significantly reducing their runtime.\")"
      ],
      "metadata": {
        "id": "mygxI3UFm20W"
      },
      "id": "mygxI3UFm20W",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}